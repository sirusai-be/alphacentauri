{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06f2c8c",
   "metadata": {},
   "source": [
    "# 1. Connecting to and Querying a DataSource (XLSX - SQLLite)\n",
    "\n",
    "This is a series of example of how you can connect Agents to different sources of data with Microsoft Azure's OpenAI Service\n",
    "\n",
    "This is the <b>Third</b> file of 3 similar scripts\n",
    "\n",
    "## Note Upfront\n",
    "If you want to your LLM model to have knowledge of your own data you can use  \n",
    "  - Fine Tune your model with your data but this is very expensive to do (CPU) and doesn't work well with data that changes.  \n",
    "  - RAG with a vectordatabase: this is suitable for documents, pictures, ..., but what about data in structured storage like databases or spreadsheets?\n",
    "  - use a connector to a database: this is what this module is about\n",
    "  \n",
    "- In this series we will show how you can set up an Agent to \n",
    "  - use Langchain agents to connect to SQL Database or CSV file\n",
    "  - via Azure OpenAI Assistants API (function calling + code interpreter) (stateful management + short term memory)\n",
    "  - via Azure OpenAI Fucntion Calling: to perform tasks based on your questions  \n",
    "- the 2nd part of this exercise connects to a SQLlite database, you have to have it installed on your environment  \n",
    "- This demo is based upon on https://learn.deeplearning.ai/courses/building-your-own-database-agent  \n",
    "\n",
    "## prereqs \n",
    "0. setup your local repo with a clone from this gitrepo. Don't forget to run the requirements.txt\n",
    "1. have a MS Azure Account; with a valid subscription  \n",
    "    - running through the course steps cost me < â‚¬0.50 but keep an eye on the costs. (portal.azure.com > search for 'Invoices' > Select 'Invoices' > Cost Management > Cost Analysis)  \n",
    "    - remove the project when no longer needed to avoid recurrent costs.      \n",
    "2. have a AI Foundry project with a deployed model\n",
    "* Create a project -> Azure AI Foundry Resource  \n",
    "    - chose a meaningful name, subscription you have setup, resource group (or create a new one), region (I typically pick Sweden Central as most of the AI Models are there)\n",
    "* Pick the right urls & credentials !!  \n",
    "    - pick the API Key and put it in your local .env with   \n",
    "    - libraries: PICK AZURE OpenAI: something like https://<project_name>-resource.openai.azure.com/  \n",
    "\n",
    "Your .env needs to look something like\n",
    "AZURE_OPENAI_API_KEY=<your_api_key>  \n",
    "AZURE_URL=https://<project_name>-resource.openai.azure.com/<br>\n",
    "\n",
    "* Deploy a model: You can pick anymodel but I work with the gpt-4.1-mini model and model version. Put that in the .env file to have all parameters in one location  \n",
    "AZURE_OPENAI_MODEL=gpt-4.1-mini  \n",
    "AZURE_OPENAI_MODEL_VERSION=2025-03-01-preview\n",
    "## 1.4 Loading a XLSX and using the build-in CODE INTERPRETER\n",
    "\n",
    "### 1.3.1 Step 1: Setting up your Azure & Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f50d21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, HTML, display\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)   # avoid the sytem set parameters to override your local the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc68452",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Azure OpenAI Configuration (CORRECT endpoint from Azure Portal)\n",
    "# endpoint = \"https://js-alphacentauri-resource.cognitiveservices.azure.com/\"\n",
    "endpoint = os.getenv(\"AZURE_URL\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "v_model = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_MODEL_VERSION\")  # Updated to latest API version for Responses API support\n",
    "\n",
    "# Get API key from environment\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# if issues, uncomment the following to validate the keys are correctly read\n",
    "# print(\"âœ… Azure OpenAI client configured\")\n",
    "# print(f\"Endpoint: {endpoint}\")\n",
    "# print(f\"Deployment: {deployment}\")\n",
    "# print(f\"API Version: {api_version}\")    \n",
    "# print(f\"Subscription Key: {subscription_key}\")\n",
    "# print(f\"API Key (1st 5 Chars): {subscription_key[:5]}...\")\n",
    "\n",
    "# Create Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c6a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm up and running, ready to assist you. To lighten the mood, hereâ€™s a joke for you:\n",
      "\n",
      "Why did the Generative AI break up with Langchain?\n",
      "\n",
      "Because it got tired of all the callbacks! ðŸ˜„\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Test the connection\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say hello and tell me you're working! But to lighten up the atmosphere, tell a joke about Generative AI and Langchain.\",\n",
    "        }\n",
    "    ],\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0b3a6",
   "metadata": {},
   "source": [
    "## 1.4: Leveraging CODE_INTERPRETER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b1f90",
   "metadata": {},
   "source": [
    "###Code Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379b9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=api_version\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c4a242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file id: assistant-CFjSeKKrBiE16hSzZHpi4n\n"
     ]
    }
   ],
   "source": [
    "# Upload the CSV file\n",
    "file_path = \"./data/synthetic_sales_data.csv\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    uploaded = client.files.create(\n",
    "        file=f,\n",
    "        purpose=\"assistants\"  # Azure currently uses \"assistants\" until official user_data support\n",
    "    )\n",
    "print(\"Uploaded file id:\", uploaded.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8557315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare query and use Responses API\n",
    "user_question = \"What's the sales total per region per year\"\n",
    "response = client.responses.create(\n",
    "    model=v_model,\n",
    "    instructions=\"\"\"You are a sales business intelligence assistant answering questions about sales data. The CSV file has been uploaded and is available for analysis. \n",
    "    Answer only with the response to the question, no other text or comments, no reasoning, no explanation. \n",
    "    Do not ask follow-up questions\"\"\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"code_interpreter\",\n",
    "            \"container\": {\"type\": \"auto\", \"file_ids\": [uploaded.id]}\n",
    "        }\n",
    "    ],\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_question\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86bd288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"resp_00640b25058dc77b006908d36bfb40819399e69a3f7e55f3a1\",\n",
      "  \"created_at\": 1762186091.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": \"You are a sales business intelligence assistant answering questions about sales data. The CSV file has been uploaded and is available for analysis. \\n    Answer only with the response to the question, no other text or comments, no reasoning, no explanation. \\n    Do not ask follow-up questions\",\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4.1-mini\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"ci_00640b25058dc77b006908d36e35908193b414e65fa8180df2\",\n",
      "      \"code\": \"import pandas as pd\\n\\n# Load the data\\nfile_path = '/mnt/data/assistant-CFjSeKKrBiE16hSzZHpi4n-synthetic_sales_data.csv'\\ndf = pd.read_csv(file_path)\\n\\n# Check the columns to understand the structure\\ndf.columns\",\n",
      "      \"container_id\": \"cntr_6908d36c802c8190882f10bb2bd918440d6eb9dbf3b9fb8f\",\n",
      "      \"outputs\": null,\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"code_interpreter_call\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ci_00640b25058dc77b006908d370541c819383d34f67237b8f07\",\n",
      "      \"code\": \"# It appears the whole header is in a single string separated by #\\n# We need to reload with proper header separation\\n\\ndf = pd.read_csv(file_path, delimiter='#')\\ndf.head()\",\n",
      "      \"container_id\": \"cntr_6908d36c802c8190882f10bb2bd918440d6eb9dbf3b9fb8f\",\n",
      "      \"outputs\": null,\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"code_interpreter_call\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ci_00640b25058dc77b006908d371b3048193931fa3dbe0201dd4\",\n",
      "      \"code\": \"# Extract the year from the Month column\\ndf['Year'] = pd.to_datetime(df['Month']).dt.year\\n\\n# Group by region and year, then sum the revenue\\nsales_total_per_region_year = df.groupby(['Region', 'Year'])['Revenue'].sum().reset_index()\\n\\nsales_total_per_region_year\",\n",
      "      \"container_id\": \"cntr_6908d36c802c8190882f10bb2bd918440d6eb9dbf3b9fb8f\",\n",
      "      \"outputs\": null,\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"code_interpreter_call\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_00640b25058dc77b006908d373342081939d8c8f52577f3764\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"| Region | Year | Revenue  |\\n|--------|------|----------|\\n| East   | 2023 | 379,388  |\\n| East   | 2024 | 255,207  |\\n| East   | 2025 | 914,767  |\\n| East   | 2026 | 1,090,312|\\n| East   | 2027 | 615,688  |\\n| East   | 2028 | 296,825  |\\n| East   | 2029 | 642,100  |\\n| East   | 2030 | 440,879  |\\n| East   | 2031 | 894,307  |\\n| North  | 2023 | 664,430  |\\n| North  | 2024 | 776,848  |\\n| North  | 2025 | 1,124,744|\\n| North  | 2026 | 425,787  |\\n| North  | 2027 | 1,152,745|\\n| North  | 2028 | 340,727  |\\n| North  | 2029 | 645,810  |\\n| North  | 2030 | 391,115  |\\n| North  | 2031 | 336,765  |\\n| South  | 2023 | 806,581  |\\n| South  | 2024 | 859,722  |\\n| South  | 2025 | 656,457  |\\n| South  | 2026 | 1,687,648|\\n| South  | 2028 | 1,013,571|\\n| South  | 2029 | 1,494,355|\\n| South  | 2030 | 522,308  |\\n| West   | 2023 | 1,252,562|\\n| West   | 2024 | 843,120  |\\n| West   | 2025 | 979,114  |\\n| West   | 2027 | 1,462,810|\\n| West   | 2028 | 197,609  |\\n| West   | 2029 | 109,376  |\\n| West   | 2030 | 1,673,645|\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"container\": {\n",
      "        \"type\": \"auto\",\n",
      "        \"file_ids\": [\n",
      "          \"assistant-CFjSeKKrBiE16hSzZHpi4n\"\n",
      "        ]\n",
      "      },\n",
      "      \"type\": \"code_interpreter\"\n",
      "    }\n",
      "  ],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"conversation\": null,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": null,\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": \"medium\"\n",
      "  },\n",
      "  \"top_logprobs\": 0,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 3796,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 1152\n",
      "    },\n",
      "    \"output_tokens\": 651,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 4447\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"content_filters\": null,\n",
      "  \"store\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f190719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Region | Year | Revenue  |\n",
      "|--------|------|----------|\n",
      "| East   | 2023 | 379,388  |\n",
      "| East   | 2024 | 255,207  |\n",
      "| East   | 2025 | 914,767  |\n",
      "| East   | 2026 | 1,090,312|\n",
      "| East   | 2027 | 615,688  |\n",
      "| East   | 2028 | 296,825  |\n",
      "| East   | 2029 | 642,100  |\n",
      "| East   | 2030 | 440,879  |\n",
      "| East   | 2031 | 894,307  |\n",
      "| North  | 2023 | 664,430  |\n",
      "| North  | 2024 | 776,848  |\n",
      "| North  | 2025 | 1,124,744|\n",
      "| North  | 2026 | 425,787  |\n",
      "| North  | 2027 | 1,152,745|\n",
      "| North  | 2028 | 340,727  |\n",
      "| North  | 2029 | 645,810  |\n",
      "| North  | 2030 | 391,115  |\n",
      "| North  | 2031 | 336,765  |\n",
      "| South  | 2023 | 806,581  |\n",
      "| South  | 2024 | 859,722  |\n",
      "| South  | 2025 | 656,457  |\n",
      "| South  | 2026 | 1,687,648|\n",
      "| South  | 2028 | 1,013,571|\n",
      "| South  | 2029 | 1,494,355|\n",
      "| South  | 2030 | 522,308  |\n",
      "| West   | 2023 | 1,252,562|\n",
      "| West   | 2024 | 843,120  |\n",
      "| West   | 2025 | 979,114  |\n",
      "| West   | 2027 | 1,462,810|\n",
      "| West   | 2028 | 197,609  |\n",
      "| West   | 2029 | 109,376  |\n",
      "| West   | 2030 | 1,673,645|\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract assistant text content from the response object\n",
    "for item in response.output:\n",
    "    # Check if this is a message (has 'role' attribute)\n",
    "    if hasattr(item, 'role') and item.role == \"assistant\":\n",
    "        # Iterate through content items\n",
    "        if hasattr(item, 'content'):\n",
    "            for content_item in item.content:\n",
    "                if hasattr(content_item, 'type') and content_item.type == \"output_text\":\n",
    "                    if hasattr(content_item, 'text'):\n",
    "                        print(content_item.text)\n",
    "                        print(\"\\n---\\n\")  # Separator between messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
